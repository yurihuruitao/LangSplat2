# LangSplat: 3D Language Gaussian Splatting

## Project Overview
LangSplat extends 3D Gaussian Splatting with language understanding capabilities, enabling 3D scene representation with semantic features. The system combines SAM (Segment Anything Model), OpenCLIP embeddings, and a custom autoencoder to create language-aware 3D Gaussians.

**Core Architecture**: Three-stage pipeline
1. **Preprocessing**: Generate 512-dim language features per image using SAM + OpenCLIP (`preprocess.py`)
2. **Autoencoder**: Compress 512-dim features → 3-dim for memory efficiency (`autoencoder/`)
3. **Training**: Optimize 3D Gaussians with language features on top of pre-trained RGB 3DGS model (`train.py`)

## Critical Workflows

### Dataset Structure Requirements
Every scene MUST follow this exact structure before training:
```
<scene_name>/
├── images/              # RGB images
├── input/              # Original images for preprocessing
├── language_features/   # 512-dim SAM+CLIP features (generated by preprocess.py)
│   ├── 00_f.npy        # Feature vectors
│   └── 00_s.npy        # Segmentation maps
├── language_features_dim3/  # 3-dim compressed features (from autoencoder)
├── output/<scene_name>/     # Pre-trained RGB 3DGS model (required!)
│   ├── chkpnt30000.pth
│   ├── cameras.json
│   └── cfg_args
└── sparse/0/           # COLMAP SfM data
    ├── cameras.bin
    ├── images.bin
    └── points3D.bin
```

### Complete Training Pipeline
Follow `process.sh` sequentially - each step depends on previous outputs:

```bash
# Step 1: Generate 512-dim language features (SAM + OpenCLIP)
python preprocess.py --dataset_path <scene_path>
# Outputs: language_features/*.npy files

# Step 2: Train scene-specific autoencoder (512→3 dims)
cd autoencoder
python train.py --dataset_path <scene_path> \
    --encoder_dims 256 128 64 32 3 \
    --decoder_dims 16 32 64 128 256 256 512 \
    --dataset_name <scene_name>
# Outputs: ckpt/<scene_name>/best_ckpt.pth

# Step 3: Generate 3-dim compressed features
python test.py --dataset_path <scene_path> --dataset_name <scene_name>
# Outputs: language_features_dim3/*.npy

# Step 4: Train LangSplat (REQUIRES pre-trained RGB 3DGS checkpoint!)
cd ..
python train.py -s <scene_path> -m output/<scene_name> \
    --start_checkpoint <scene_path>/output/<scene_name>/chkpnt30000.pth \
    --feature_level 3  # 1, 2, or 3 for different feature resolutions

# Step 5: Render
python render.py -m output/<scene_name>_3 --include_feature
```

### Pre-trained RGB Model is MANDATORY
Before training LangSplat, you MUST have a trained RGB 3D Gaussian Splatting model from [3DGS](https://github.com/graphdeco-inria/gaussian-splatting). The system fine-tunes this model by:
- Freezing RGB Gaussian parameters (xyz, features, scaling, rotation, opacity)
- Adding and optimizing only `_language_feature` (N×3 tensor) per Gaussian

## Key Code Patterns

### Dual Training Modes
The codebase switches between RGB and language feature training via `opt.include_feature`:

**RGB Training** (`include_feature=False`):
- Optimizes: position, SH features, opacity, scaling, rotation
- Loss: L1 + SSIM on RGB images
- Densification enabled (clone/split Gaussians)

**Language Feature Training** (`include_feature=True`):
- Freezes all RGB parameters, optimizes only `_language_feature` 
- Loss: L1 between rendered and ground truth 3-dim language features
- No densification (preserves Gaussian structure)

```python
# In train.py - loss computation switches based on mode
if opt.include_feature:
    gt_language_feature, mask = viewpoint_cam.get_language_feature(
        language_feature_dir=dataset.lf_path, 
        feature_level=dataset.feature_level
    )
    Ll1 = l1_loss(language_feature*mask, gt_language_feature*mask)
else:
    gt_image = viewpoint_cam.original_image.cuda()
    Ll1 = l1_loss(image, gt_image)
    loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))
```

### Gaussian Model State Management
`scene/gaussian_model.py` handles checkpoints with/without language features:
- 12 tensors = RGB-only checkpoint
- 13 tensors = checkpoint with `_language_feature`

When loading RGB checkpoint for language training, the system initializes `_language_feature` as zeros in `training_setup()`.

### Feature Level System
`--feature_level` (1/2/3) controls multi-scale language features:
- Uses different scales of SAM segmentation (default/s/m/l)
- Level 3 typically gives best results
- Model path automatically appends `_<level>` suffix

## Submodules & Custom Extensions

### Critical CUDA Extensions (in `submodules/`)
1. **langsplat-rasterization**: Modified 3DGS rasterizer with language feature support
   - Accepts `language_feature_precomp` parameter
   - Returns both RGB and language feature renderings
2. **segment-anything-langsplat**: Modified SAM for multi-scale segmentation
3. **simple-knn**: KNN for Gaussian initialization

**Installation**: These are pip-installed from local paths in `environment.yml`:
```yaml
pip:
  - submodules/segment-anything-langsplat
  - submodules/langsplat-rasterization
  - submodules/simple-knn
```

## Environment & Dependencies

### Core Requirements
- **CUDA 11.6+** (CUDA 11.8 tested, critical for extensions)
- **Python 3.7** (pinned for PyTorch 1.12.1 compatibility)
- **24GB VRAM** for full-resolution training
- OpenCLIP for language embeddings
- SAM checkpoint: `ckpts/sam_vit_h_4b8939.pth`

### Setup
```bash
conda env create --file environment.yml
conda activate langsplat
```

## Evaluation
Supports 3D object localization and semantic segmentation on LERF dataset:
```bash
cd eval
# Requires: 1) rendered language features, 2) trained autoencoder decoder
python evaluate_iou_loc.py \
    --dataset_name <scene> \
    --feat_dir ../output \
    --ae_ckpt_dir ../autoencoder/ckpt \
    --encoder_dims 256 128 64 32 3 \
    --decoder_dims 16 32 64 128 256 256 512 \
    --json_folder <path_to_lerf_ovs/label>
```

## Common Pitfalls
- **Missing pre-trained RGB checkpoint**: Training will fail with "checkpoint missing" error
- **Incorrect dataset structure**: Ensure `language_features/` and `sparse/0/` exist before training
- **Feature level mismatch**: Model output path includes feature level; use consistent `--feature_level` for train/render
- **Autoencoder dimensions**: Always use `--encoder_dims 256 128 64 32 3 --decoder_dims 16 32 64 128 256 256 512` (project standard)

## File Naming Conventions
- Language features: `<idx>_f.npy` (features), `<idx>_s.npy` (segmentation maps)
- Checkpoints: `chkpnt<iteration>.pth` (e.g., `chkpnt30000.pth`)
- Model outputs: `output/<scene_name>_<feature_level>/`
